# AutoencoderFinal

In order to determine the optimal configuration of hyperparameters that would result in the highest accuracy, this experiment employed a combination of 5 different activation functions, single, double, and triple hidden layers, each constructed of a varied number of neurons. The autoencoder model benchmarked was utilized to extract important features from protein sequences that would later be used for the decoding of a testing set of randomized protein sequences. The sequence length of the protein sequences was 43 characters. Each character described 1 of 20 amino acids, plus an additional character (dash line, ‘-’) to account for gaps of indeterminate length in the sequence, giving a total of 21 possible characters. The sequences were then one-hot encoded by multiplying the sequence length (43) by the sequence depth (21) so that it was ensured that the model ran at its fastest capacity. The training, testing, and validation split was 70%, 20%, and 10%, respectively, and featured a total of 12,459 samples. Accuracy was measured by how exact the model performed in the reconstruction of the testing set of protein sequences. Overall, the model accuracy tended to increase proportionally to the number of neurons present in a hidden layer. However, experimental results dictated that as the number of hidden layers increased, the accuracy of the model decreased. Aside from ReLU, all the activation functions depicted a similar distribution across the hidden layer modifications and accuracy. Given that ReLU consistently had a significant underperformance, it could indicate that there may have been a large negative bias term that negatively affected the way ReLU processed gradients for its weights.
